{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ---------- Config ----------\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"  # or another compatible model\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "MAX_NEW_TOKENS = 5\n",
        "TEMPERATURE = 0.2  # small nonzero helps reduce \"always No\" bias\n",
        "OUT_FILE = \"bluff_labeled.jsonl\"\n",
        "CHECKPOINT_FILE = \"bluff_checkpoint.json\"\n",
        "\n",
        "SAVE_EVERY_BATCHES = 20\n",
        "\n",
        "# Load your data here\n",
        "with open(\"llm_raise.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------- Model Setup ----------\n",
        "print(f\"Loading model {MODEL_ID}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16).to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Model loaded.\\n\")\n",
        "\n",
        "# ---------- Helper Functions ----------\n",
        "def save_progress(index):\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        json.dump({\"last_index\": index}, f)\n",
        "\n",
        "def load_progress():\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return json.load(f).get(\"last_index\", 0)\n",
        "    return 0\n",
        "\n",
        "def append_jsonl(outfile, records):\n",
        "    with open(outfile, \"a\", encoding=\"utf-8\") as f:\n",
        "        for rec in records:\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def clean_instruction(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove any trailing 'Your optimal action is' sections or redundant text.\n",
        "    \"\"\"\n",
        "    cleaned = re.split(r\"\\n\\nNow it is your turn.*\", instruction, maxsplit=1)[0]\n",
        "    pot_match = re.search(r\"pot size is ([\\d.]+) chips\", instruction)\n",
        "    if pot_match:\n",
        "        pot_size = pot_match.group(1)\n",
        "        cleaned += f\"\\nThe current pot size is {pot_size} chips.\"\n",
        "    return cleaned.strip()\n",
        "\n",
        "def build_bluff_prompt(instruction, llm_move):\n",
        "    \"\"\"\n",
        "    Build a short, balanced prompt that directly requests Yes/No output.\n",
        "    Includes a few-shot context to encourage realistic bluff judgment.\n",
        "    \"\"\"\n",
        "    first_word = llm_move.split()[0] if llm_move.strip() else \"Raise\"\n",
        "    clean_instr = clean_instruction(instruction)\n",
        "\n",
        "    few_shot_examples = \"\"\"Example 1:\n",
        "Pot is 50 chips. Player has a strong hand (Ace, King on Ace-high board).\n",
        "Player raises.\n",
        "Is this a bluff? No\n",
        "\n",
        "Example 2:\n",
        "Pot is 50 chips. Player has 7-2 offsuit on Ace-high board and raises after opponent bets.\n",
        "Is this a bluff? Yes\n",
        "\"\"\"\n",
        "\n",
        "    return f\"\"\"{few_shot_examples}\n",
        "\n",
        "Definition: A bluff is when a player bets or raises with a weak hand in order to make the opponent fold a better hand.\n",
        "\n",
        "{clean_instr}\n",
        "\n",
        "The player decided to: {first_word}.\n",
        "Is this a bluff? Reply with only 'Yes' or 'No'.\"\"\"\n",
        "\n",
        "def normalize_yes_no(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize LLM output to just 'Yes' or 'No'.\n",
        "    \"\"\"\n",
        "    text = text.strip().lower()\n",
        "    if text.startswith(\"yes\"):\n",
        "        return \"Yes\"\n",
        "    if text.startswith(\"no\"):\n",
        "        return \"No\"\n",
        "    return text  # fallback if unclear\n",
        "\n",
        "# ---------- Main Loop ----------\n",
        "start_idx = load_progress()\n",
        "N = len(train_data)\n",
        "print(f\"Bluff labeling: {N} records, starting from index {start_idx}\")\n",
        "\n",
        "i = start_idx\n",
        "batches_done = 0\n",
        "\n",
        "try:\n",
        "    while i < N:\n",
        "        end = min(i + BATCH_SIZE, N)\n",
        "        batch = train_data[i:end]\n",
        "\n",
        "        prompts = [build_bluff_prompt(r[\"instruction\"], r[\"llm_move\"]) for r in batch]\n",
        "\n",
        "        # Debug preview\n",
        "        if batches_done == 0:\n",
        "            print(\"\\n--- DEBUG EXAMPLE ---\")\n",
        "            print(\"Prompt:\\n\", prompts[0][:800])\n",
        "            print(\"--------------------\\n\")\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=MAX_NEW_TOKENS,\n",
        "                    pad_token_id=model.config.pad_token_id,\n",
        "                    do_sample=True if TEMPERATURE > 0 else False,\n",
        "                    temperature=TEMPERATURE if TEMPERATURE > 0 else None,\n",
        "                )\n",
        "\n",
        "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            decoded = decoded[:len(prompts)]\n",
        "\n",
        "            records = []\n",
        "            for orig, full_text, prompt in zip(batch, decoded, prompts):\n",
        "                # Remove prompt portion if model echoed it\n",
        "                clean_text = full_text[len(prompt):].strip() if full_text.startswith(prompt) else full_text.strip()\n",
        "\n",
        "                clean_answer = normalize_yes_no(clean_text)\n",
        "\n",
        "                records.append({\n",
        "                    **orig,\n",
        "                    \"is_bluff\": clean_answer\n",
        "                })\n",
        "\n",
        "            append_jsonl(OUT_FILE, records)\n",
        "\n",
        "            # Debug sample\n",
        "            if batches_done < 2:\n",
        "                print(\"--- DEBUG OUTPUT ---\")\n",
        "                print(f\"Raw Model Output:\\n{decoded[0]}\\n\")\n",
        "                print(f\"Cleaned Model Response: {clean_text}\")\n",
        "                print(f\"Final is_bluff: {clean_answer}\")\n",
        "                print(\"--------------------\\n\")\n",
        "\n",
        "            i = end\n",
        "            batches_done += 1\n",
        "\n",
        "            if batches_done % SAVE_EVERY_BATCHES == 0:\n",
        "                save_progress(i)\n",
        "                print(f\"Checkpoint saved at index {i}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            err_str = str(e).lower()\n",
        "            print(f\"Runtime error at batch starting {i}: {e}\")\n",
        "            if \"out of memory\" in err_str or \"cuda\" in err_str:\n",
        "                torch.cuda.empty_cache()\n",
        "                if BATCH_SIZE <= 1:\n",
        "                    raise\n",
        "                old_bs = BATCH_SIZE\n",
        "                BATCH_SIZE = max(1, BATCH_SIZE // 2)\n",
        "                print(f\"Reducing batch size from {old_bs} to {BATCH_SIZE} and retrying at index {i}\")\n",
        "                continue\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Interrupted by user — saving progress.\")\n",
        "    save_progress(i)\n",
        "    raise\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e} — saving progress at index {i}\")\n",
        "    save_progress(i)\n",
        "    raise\n",
        "\n",
        "else:\n",
        "    print(\"Completed all prompts. Final checkpointing...\")\n",
        "    save_progress(i)"
      ],
      "metadata": {
        "id": "LUb2sgdp-FzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}