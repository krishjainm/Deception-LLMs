{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2wqgRtGQ6_G"
      },
      "outputs": [],
      "source": [
        "!pip install openai aiohttp tqdm nest_asyncio pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "uZ2aqXiORBnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ay3M1GkRQX0",
        "outputId": "358ae74a-6c22-4af4-c254-e9ff5f34fe70"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_instruction(instr: str) -> str:\n",
        "    \"\"\"Removes noisy prefixes or trailing text.\"\"\"\n",
        "    if not instr:\n",
        "        return \"\"\n",
        "    return instr.split(\"Now it is your turn\")[0].strip()\n",
        "\n",
        "def build_bluff_prompt(instruction: str, llm_move: str) -> str:\n",
        "    \"\"\"Builds concise yes/no prompt.\"\"\"\n",
        "    first_word = llm_move.split()[0] if llm_move.strip() else \"Raise\"\n",
        "    clean_instr = clean_instruction(instruction)\n",
        "    return f\"\"\"{clean_instr}\n",
        "\n",
        "The player decided to: {first_word}.\n",
        "Is this a bluff? Reply with only 'Yes' or 'No'.\"\"\"\n"
      ],
      "metadata": {
        "id": "cA0HqUTJRVmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_bluff_label(session, prompt, model=\"gpt-4o\"):\n",
        "    \"\"\"Queries OpenAI model and returns 'Yes' or 'No' only.\"\"\"\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert poker analyst.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "                max_tokens=5,\n",
        "            )\n",
        "            result = response.choices[0].message.content.strip()\n",
        "            if result not in [\"Yes\", \"No\"]:\n",
        "                # Fallback if LLM gives explanation\n",
        "                result = \"Yes\" if \"yes\" in result.lower() else \"No\"\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            if attempt < 2:\n",
        "                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "            else:\n",
        "                return \"Error\""
      ],
      "metadata": {
        "id": "mg49uqIlRYLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"/content/llm_raise.jsonl\"\n",
        "\n",
        "with open(train_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Loaded {len(train_data)} records\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IBuvwLbRpUl",
        "outputId": "1f2086a3-7bb6-4a9a-ec97-037c9cefc6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 70144 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def process_batch(batch, output_file, session):\n",
        "    tasks = []\n",
        "    for row in batch:\n",
        "        prompt = build_bluff_prompt(row[\"instruction\"], row[\"llm_move\"])\n",
        "        tasks.append(get_bluff_label(session, prompt))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    records = []\n",
        "\n",
        "    for row, is_bluff in zip(batch, results):\n",
        "        row[\"is_bluff\"] = is_bluff\n",
        "        records.append(row)\n",
        "\n",
        "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        for rec in records:\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "async def main():\n",
        "    output_file = \"is_bluff_openai_gpt-4o.jsonl\"\n",
        "    batch_size = 100  # adjust based on rate limits\n",
        "\n",
        "    connector = aiohttp.TCPConnector(limit=20)\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        for i in tqdm(range(0, len(train_data), batch_size)):\n",
        "            batch = train_data[i:i + batch_size]\n",
        "            await process_batch(batch, output_file, session)\n",
        "\n",
        "    print(f\"\\n✅ Done! Results saved to {output_file}\")"
      ],
      "metadata": {
        "id": "RLJGun4TRygu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6xdE0TrR2C_",
        "outputId": "596d6786-88b0-4e0d-f41f-77557d17cbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 702/702 [44:55<00:00,  3.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done! Results saved to is_bluff_openai_gpt-4o.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"is_bluff_openai_gpt-4o.jsonl\", lines=True)\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df[\"is_bluff\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsMrmvI3R5Ci",
        "outputId": "6bd1c797-113b-45d6-9192-19dc3d100e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label distribution:\n",
            "is_bluff\n",
            "Yes    36907\n",
            "No     33237\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "input_path = \"is_bluff_openai_gpt-4o.jsonl\"\n",
        "output_path = \"is_bluff_yes_gpt-4o.jsonl\"\n",
        "\n",
        "# Load JSONL file into a DataFrame\n",
        "df = pd.read_json(input_path, lines=True)\n",
        "\n",
        "# Filter where is_bluff == \"Yes\"\n",
        "df_yes = df[df[\"is_bluff\"].str.lower() == \"yes\"]\n",
        "\n",
        "# Save filtered rows to new JSONL file\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in df_yes.iterrows():\n",
        "        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Saved {len(df_yes)} bluff rows to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0doBVxDizV0",
        "outputId": "24e8cfc5-1407-4ab5-ebb3-ff26f8011c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 36907 bluff rows to is_bluff_yes_gpt-4o.jsonl\n"
          ]
        }
      ]
    }
  ]
}